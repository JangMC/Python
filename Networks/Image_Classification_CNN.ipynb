{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will assume that I need to provide advice to a website provider who is looking for tools to automatically label images provided by end users. Recommendations to management about image classification will maximize accuracy over processing time. \n",
    "\n",
    "To determine a recommendation, I will use 1000 images of cats and 1000 images of dogs at two resolutions (64x64) and (128x128). This will be the basis to employ a 2x2 factorial design with resolution and color/grayscale as the two experimental factors. To keep this simple, I will not modify elements of the models (e.g. layers, filters, etc.), but keep everything constant as to not confound the experient. The results seen below will reveal that simple images (low resolution and grayscale) work better for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# starting with code from run-cats-dogs-jump-start-dnn-v002.py\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# set-up needed modules\n",
    "import os, random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "#CatsDogs Old dimensions\n",
    "height = 64\n",
    "width = 64   \n",
    "    \n",
    "\n",
    "# Read in cats and dogs files to create training data\n",
    "# used the output from run-cats-dogs-data-prep-v001.py\n",
    "cats_1000_64_64_1 = np.load('cats_dogs_64-128/cats_1000_64_64_1.npy')\n",
    "dogs_1000_64_64_1 = np.load('cats_dogs_64-128/dogs_1000_64_64_1.npy')\n",
    "\n",
    "cats_1000_64_64_3 = np.load('cats_dogs_64-128/cats_1000_64_64_3.npy')\n",
    "dogs_1000_64_64_3 = np.load('cats_dogs_64-128/dogs_1000_64_64_3.npy')\n",
    "\n",
    "cats_1000_128_128_1 = np.load('cats_dogs_64-128/cats_1000_128_128_1.npy')\n",
    "dogs_1000_128_128_1 = np.load('cats_dogs_64-128/dogs_1000_128_128_1.npy')\n",
    "\n",
    "cats_1000_128_128_3 = np.load('cats_dogs_64-128/cats_1000_128_128_3.npy')\n",
    "dogs_1000_128_128_3 = np.load('cats_dogs_64-128/dogs_1000_128_128_3.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128, 128, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review shaoes of the loaded data\n",
    "dogs_1000_128_128_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an idea of the images were dealing with\n",
    "def show_grayscale_image(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "# Examine first cat and first dog grayscale images\n",
    "show_grayscale_image(cats_1000_64_64_1[0,:,:,0])\n",
    "show_grayscale_image(dogs_1000_64_64_1[0,:,:,0])\n",
    "\n",
    "show_grayscale_image(cats_1000_64_64_3[0,:,:,0])\n",
    "show_grayscale_image(dogs_1000_64_64_3[0,:,:,0])\n",
    "\n",
    "show_grayscale_image(cats_1000_128_128_1[0,:,:,0])\n",
    "show_grayscale_image(dogs_1000_128_128_1[0,:,:,0])\n",
    "\n",
    "show_grayscale_image(cats_1000_128_128_3[0,:,:,0])\n",
    "show_grayscale_image(dogs_1000_128_128_3[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran cell, but deleted results to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the cats and dogs together\n",
    "# Stack the numpy arrays for the inputs\n",
    "X_cat_dog64_1 = np.concatenate((cats_1000_64_64_1, dogs_1000_64_64_1), axis = 0) \n",
    "\n",
    "X_cat_dog64_3 = np.concatenate((cats_1000_64_64_3, dogs_1000_64_64_3), axis = 0) \n",
    "\n",
    "X_cat_dog128_1 = np.concatenate((cats_1000_128_128_1, dogs_1000_128_128_1), axis = 0) \n",
    "\n",
    "X_cat_dog128_3 = np.concatenate((cats_1000_128_128_3, dogs_1000_128_128_3), axis = 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 128, 128, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to see the arrays have been brought together\n",
    "X_cat_dog64_1.shape\n",
    "X_cat_dog64_3.shape\n",
    "X_cat_dog128_1.shape\n",
    "X_cat_dog128_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select ome cat and one dog image randomly to ensure\n",
    "# data is brought together\n",
    "show_grayscale_image(X_cat_dog64_1[900,:,:,0])\n",
    "show_grayscale_image(X_cat_dog64_1[1104,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran cell, but deleted results to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels to be used 1000 cats = 0 1000 dogs = 1\n",
    "y_cat_dog64_1 = np.concatenate((np.zeros((1000), dtype = np.int32), \n",
    "                      np.ones((1000), dtype = np.int32)), axis = 0)\n",
    "\n",
    "y_cat_dog64_3 = np.concatenate((np.zeros((1000), dtype = np.int32), \n",
    "                      np.ones((1000), dtype = np.int32)), axis = 0)\n",
    "\n",
    "y_cat_dog128_1 = np.concatenate((np.zeros((1000), dtype = np.int32), \n",
    "                      np.ones((1000), dtype = np.int32)), axis = 0)\n",
    "\n",
    "y_cat_dog128_3 = np.concatenate((np.zeros((1000), dtype = np.int32), \n",
    "                      np.ones((1000), dtype = np.int32)), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "X_cat_dog64_1 *= 255.0/X_cat_dog64_1.max() \n",
    "X_cat_dog64_3 *= 255.0/X_cat_dog64_3.max() \n",
    "X_cat_dog128_1 *= 255.0/X_cat_dog128_1.max() \n",
    "X_cat_dog128_3 *= 255.0/X_cat_dog128_3.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train64_1, X_test64_1, y_train64_1, y_test64_1 = \\\n",
    "    train_test_split(X_cat_dog64_1, y_cat_dog64_1, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, val, test set will be 60%, 20%, 20% of the dataset respectively  \n",
    "X_train64_1, X_val64_1, y_train64_1, y_val64_1 = \\\n",
    "    train_test_split(X_train64_1, y_train64_1, test_size=0.25, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 64, 64, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test64_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CNN for (64x64) grayscale photos (1 channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when val_loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack of 3 convolution layers with a ReLU activation\n",
    "# followed by max-pooling layers\n",
    "model64_1 = Sequential()\n",
    "# 1: Convolution layer with 32 filters, each 3x3x3\n",
    "model64_1.add(Conv2D(34, (3, 3), input_shape=(64, 64, 1)))\n",
    "model64_1.add(Activation('relu'))\n",
    "model64_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 2: Convolution layer with 32 filters, each 3x3x3\n",
    "model64_1.add(Conv2D(34, (3, 3)))\n",
    "model64_1.add(Activation('relu'))\n",
    "model64_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 3: Convolution layer with 64 filters, each 3x3x3\n",
    "model64_1.add(Conv2D(64, (3, 3)))\n",
    "model64_1.add(Activation('relu'))\n",
    "model64_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add two fully-connected layers\n",
    "# sigmoid activation for a binary classification\n",
    "# binary_crossentropy loss to train model.\n",
    "\n",
    "model64_1.add(Flatten()) \n",
    "model64_1.add(Dense(64))\n",
    "model64_1.add(Activation('relu'))\n",
    "model64_1.add(Dropout(0.5))\n",
    "model64_1.add(Dense(1))\n",
    "model64_1.add(Activation('sigmoid'))\n",
    "\n",
    "model64_1.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 16s 13ms/step - loss: 6.3981 - acc: 0.5233 - val_loss: 7.3328 - val_acc: 0.4950\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 1.3873 - acc: 0.5075 - val_loss: 0.7272 - val_acc: 0.5225\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 15s 13ms/step - loss: 0.6965 - acc: 0.5392 - val_loss: 0.6915 - val_acc: 0.5375\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 15s 13ms/step - loss: 0.6977 - acc: 0.5825 - val_loss: 0.7201 - val_acc: 0.5250\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.7073 - acc: 0.5575 - val_loss: 0.7168 - val_acc: 0.5025\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 13s 11ms/step - loss: 0.6830 - acc: 0.5758 - val_loss: 0.6877 - val_acc: 0.6075\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 0.6497 - acc: 0.6108 - val_loss: 0.7003 - val_acc: 0.5650\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 0.7778 - acc: 0.6067 - val_loss: 0.7613 - val_acc: 0.5650\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 0.6430 - acc: 0.6183 - val_loss: 0.7028 - val_acc: 0.5550\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 15s 13ms/step - loss: 0.6214 - acc: 0.6375 - val_loss: 0.7089 - val_acc: 0.5925\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 0.6029 - acc: 0.6567 - val_loss: 0.7787 - val_acc: 0.5750\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "start_time64_1 = time.clock()\n",
    "trained_model64_1 = model64_1.fit(X_train64_1, y_train64_1, epochs=25, \n",
    "                                  batch_size=100,validation_data= (X_val64_1,y_val64_1),\n",
    "                                  callbacks= [early_stopping])\n",
    "end_time64_1 = time.clock()\n",
    "runtime64_1 = end_time64_1 - start_time64_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6176470588235293, 61.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on test data and assess accuracy\n",
    "#get F1 score\n",
    "prediction64_1 = model64_1.predict(X_test64_1)\n",
    "f164_1 = f1_score(y_test64_1, prediction64_1 > 0.5)\n",
    "acc64_1 = accuracy_score(y_test64_1, prediction64_1 > 0.5)\n",
    "acc64_1 = (acc64_1*100)\n",
    "(f164_1, acc64_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run process for (64x64) color photos (3 channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train64_3, X_test64_3, y_train64_3, y_test64_3 = \\\n",
    "    train_test_split(X_cat_dog64_3, y_cat_dog64_3, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validation, test set will be 60%, 20%, 20% of the dataset respectively \n",
    "X_train64_3, X_val64_3, y_train64_3, y_val64_3 = \\\n",
    "    train_test_split(X_train64_3, y_train64_3, test_size=0.25, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack of 3 convolution layers with a ReLU activation\n",
    "# followed by max-pooling layers\n",
    "model64_3 = Sequential()\n",
    "# 1: Convolution layer with 32 filters, each 3x3x3\n",
    "model64_3.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3)))\n",
    "model64_3.add(Activation('relu'))\n",
    "model64_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 2: Convolution layer with 32 filters, each 3x3x3\n",
    "model64_3.add(Conv2D(32, (3, 3)))\n",
    "model64_3.add(Activation('relu'))\n",
    "model64_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 1: Convolution layer with 64 filters, each 3x3x3\n",
    "model64_3.add(Conv2D(64, (3, 3)))\n",
    "model64_3.add(Activation('relu'))\n",
    "model64_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add two fully-connected layers\n",
    "# sigmoid activation for a binary classification\n",
    "# binary_crossentropy loss to train model\n",
    "\n",
    "model64_3.add(Flatten())\n",
    "model64_3.add(Dense(64))\n",
    "model64_3.add(Activation('relu'))\n",
    "model64_3.add(Dropout(0.5))\n",
    "model64_3.add(Dense(1))\n",
    "model64_3.add(Activation('sigmoid'))\n",
    "\n",
    "model64_3.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 7.2494 - acc: 0.5000 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 15s 12ms/step - loss: 2.6286 - acc: 0.5325 - val_loss: 0.6978 - val_acc: 0.5350\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 14s 12ms/step - loss: 0.6993 - acc: 0.5608 - val_loss: 0.6784 - val_acc: 0.5925\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 14s 12ms/step - loss: 0.6710 - acc: 0.5917 - val_loss: 0.7098 - val_acc: 0.5175\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 14s 11ms/step - loss: 0.6638 - acc: 0.6183 - val_loss: 0.6725 - val_acc: 0.6150\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 14s 11ms/step - loss: 0.6673 - acc: 0.5908 - val_loss: 0.7111 - val_acc: 0.5750\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 14s 11ms/step - loss: 0.6650 - acc: 0.6108 - val_loss: 0.7912 - val_acc: 0.5450\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 14s 12ms/step - loss: 0.6869 - acc: 0.6225 - val_loss: 0.7661 - val_acc: 0.5350\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 14s 12ms/step - loss: 0.6286 - acc: 0.6517 - val_loss: 0.7125 - val_acc: 0.5525\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 14s 12ms/step - loss: 0.6522 - acc: 0.6600 - val_loss: 0.6986 - val_acc: 0.5950\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "#measure the processing time\n",
    "start_time64_3 = time.clock()\n",
    "trained_model64_3 = model64_3.fit(X_train64_3, y_train64_3, epochs=25, \n",
    "                                  batch_size=100,validation_data= (X_val64_3,y_val64_3),\n",
    "                                  callbacks= [early_stopping])\n",
    "end_time64_3 = time.clock()\n",
    "runtime64_3 = end_time64_3 - start_time64_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on test data and assess accuracy\n",
    "prediction64_3 = model64_3.predict(X_test64_3)\n",
    "acc64_3 = accuracy_score(y_test64_3, prediction64_3 > 0.5)\n",
    "acc64_3 = (acc64_3 *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run process for (128x128) grayscale photos (1 channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train128_1, X_test128_1, y_train128_1, y_test128_1 = \\\n",
    "    train_test_split(X_cat_dog128_1, y_cat_dog128_1, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, val, test set will be 60%, 20%, 20% of the dataset respectively \n",
    "X_train128_1, X_val128_1, y_train128_1, y_val128_1 = \\\n",
    "    train_test_split(X_train128_1, y_train128_1, test_size=0.25, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model128_1 = Sequential()\n",
    "\n",
    "# stack of 3 convolution layers with a ReLU activation\n",
    "# followed by max-pooling layers\n",
    "\n",
    "# 1: Convolution layer with 32 filters, each 3x3x3\n",
    "model128_1.add(Conv2D(32, (3, 3), input_shape=(128, 128, 1)))\n",
    "model128_1.add(Activation('relu'))\n",
    "model128_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 2: Convolution layer with 32 filters, each 3x3x3\n",
    "model128_1.add(Conv2D(32, (3, 3)))\n",
    "model128_1.add(Activation('relu'))\n",
    "model128_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3: Convolution layer with 64 filters, each 3x3x3\n",
    "model128_1.add(Conv2D(64, (3, 3)))\n",
    "model128_1.add(Activation('relu'))\n",
    "model128_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add two fully-connected layers\n",
    "# sigmoid activation for a binary classification\n",
    "# binary_crossentropy loss to train model\n",
    "\n",
    "model128_1.add(Flatten()) \n",
    "model128_1.add(Dense(64))\n",
    "model128_1.add(Activation('relu'))\n",
    "model128_1.add(Dropout(0.5))\n",
    "model128_1.add(Dense(1))\n",
    "model128_1.add(Activation('sigmoid'))\n",
    "\n",
    "model128_1.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 54s 45ms/step - loss: 8.0738 - acc: 0.4883 - val_loss: 7.8915 - val_acc: 0.5050\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 51s 42ms/step - loss: 7.8915 - acc: 0.5050 - val_loss: 7.8915 - val_acc: 0.5050\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 51s 43ms/step - loss: 7.8915 - acc: 0.5050 - val_loss: 7.8915 - val_acc: 0.5050\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 50s 42ms/step - loss: 7.8915 - acc: 0.5050 - val_loss: 7.8915 - val_acc: 0.5050\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 51s 42ms/step - loss: 7.8915 - acc: 0.5050 - val_loss: 7.8915 - val_acc: 0.5050\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 52s 43ms/step - loss: 7.7989 - acc: 0.5108 - val_loss: 7.8915 - val_acc: 0.5050\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "#measure the processing time\n",
    "start_time128_1 = time.clock()\n",
    "trained_model128_1 = model128_1.fit(X_train128_1, y_train128_1, epochs=25, \n",
    "                                  batch_size=100,validation_data= (X_val128_1,y_val128_1),\n",
    "                                   callbacks= [early_stopping])\n",
    "end_time128_1 = time.clock()\n",
    "runtime128_1 = end_time128_1 - start_time128_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on test data and assess accuracy\n",
    "prediction128_1 = model128_1.predict(X_test128_1)\n",
    "acc128_1 = accuracy_score(y_test128_1, prediction128_1 > 0.5)\n",
    "acc128_1 = (acc128_1 *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run process for (128x128) color photos (3 channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train128_3, X_test128_3, y_train128_3, y_test128_3 = \\\n",
    "    train_test_split(X_cat_dog128_3, y_cat_dog128_3, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, val, test set will be 60%, 20%, 20% of the dataset respectively  \n",
    "X_train128_3, X_val128_3, y_train128_3, y_val128_3 = \\\n",
    "    train_test_split(X_train128_3, y_train128_3, test_size=0.25, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack of 3 convolution layers with a ReLU activation\n",
    "# followed by max-pooling layers\n",
    "model128_3 = Sequential()\n",
    "# 1: Convolution layer with 32 filters, each 3x3x3\n",
    "model128_3.add(Conv2D(32, (3, 3), input_shape=(128, 128, 3)))\n",
    "model128_3.add(Activation('relu'))\n",
    "model128_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 2: Convolution layer with 32 filters, each 3x3x3\n",
    "model128_3.add(Conv2D(32, (3, 3)))\n",
    "model128_3.add(Activation('relu'))\n",
    "model128_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 3: Convolution layer with 64 filters, each 3x3x3\n",
    "model128_3.add(Conv2D(64, (3, 3)))\n",
    "model128_3.add(Activation('relu'))\n",
    "model128_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add two fully-connected layers\n",
    "# sigmoid activation for a binary classification\n",
    "# binary_crossentropy loss to train model\n",
    "\n",
    "model128_3.add(Flatten()) \n",
    "model128_3.add(Dense(64))\n",
    "model128_3.add(Activation('relu'))\n",
    "model128_3.add(Dropout(0.5))\n",
    "model128_3.add(Dense(1))\n",
    "model128_3.add(Activation('sigmoid'))\n",
    "\n",
    "model128_3.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 61s 51ms/step - loss: 7.9841 - acc: 0.5000 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 61s 51ms/step - loss: 8.1396 - acc: 0.4950 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 59s 49ms/step - loss: 8.1396 - acc: 0.4950 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 61s 51ms/step - loss: 8.1396 - acc: 0.4950 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 60s 50ms/step - loss: 8.1396 - acc: 0.4950 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 62s 52ms/step - loss: 8.1396 - acc: 0.4950 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "#measure the processing time\n",
    "start_time128_3 = time.clock()\n",
    "trained_model128_3 = model128_3.fit(X_train128_3, y_train128_3, epochs=25, \n",
    "                                  batch_size=100,validation_data= (X_val128_3,y_val128_3), \n",
    "                                callbacks= [early_stopping])\n",
    "end_time128_3 = time.clock()\n",
    "runtime128_3 = end_time128_3 - start_time128_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on test data and assess accuracy\n",
    "prediction128_3 = model128_3.predict(X_test128_3)\n",
    "acc128_3 = accuracy_score(y_test128_3, prediction128_3)\n",
    "acc128_3 = (acc128_3 *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "64x64 grayscale Processing time (seconds): 157.484248\n",
      "\n",
      "64x64 color Processing time (seconds): 141.059158\n",
      "\n",
      "128x128 grayscale Processing time (seconds): 309.712366\n",
      "\n",
      "128x128 color Processing time (seconds): 364.464930\n",
      "\n",
      "64x64 grayscale Test Accuracy: 61%\n",
      "\n",
      "64x64 color Test Accuracy: 60%\n",
      "\n",
      "128x128 grayscale Test Accuracy: 48%\n",
      "\n",
      "128x128 color Test Accuracy: 52%\n"
     ]
    }
   ],
   "source": [
    "print (\"\\n64x64 grayscale Processing time (seconds): %f\" % runtime64_1)\n",
    "print (\"\\n64x64 color Processing time (seconds): %f\" % runtime64_3)\n",
    "print (\"\\n128x128 grayscale Processing time (seconds): %f\" % runtime128_1)\n",
    "print (\"\\n128x128 color Processing time (seconds): %f\" % runtime128_3)\n",
    "print (\"\\n64x64 grayscale Test Accuracy: %1.f%%\" % acc64_1)\n",
    "print (\"\\n64x64 color Test Accuracy: %1.f%%\" % acc64_3)\n",
    "print (\"\\n128x128 grayscale Test Accuracy: %1.f%%\" % acc128_1)\n",
    "print (\"\\n128x128 color Test Accuracy: %1.f%%\" % acc128_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A summary table for the 2x2 experiment is seen below. The same CNN was leveraged for each experiment with the only changes occuring between resolution and color/grayscale so we can know what elements are impacting accuracy. The CNN inlcudes a stack of 3 convolution layers (2 with 32 filters and 1 with 64 filters) with a ReLU activation, followed by max-pooling layers.  It also includes 2 fully-connected layers with sigmoid activation for a binary classification and binary_crossentropy loss to train the model.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "For this convolutional neural network, simple images work best. The team should not necessarily be stressed about needing high resolution, color images from user submissions to run a model, as this model works best with (64x64) grayscale images. If users do choose to submit (64x64) color photos, the model will predict similarly as accuracy does not really drop (61% accurate with grayscale images vs. 60% accurate with color images). This model also is pretty efficient, taking less time to run than other models leveraging higher resoltion images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions binary: \n",
    "# cats = 0 dogs = 1\n",
    "prediction64_1[prediction64_1 > 0.5] = 1\n",
    "prediction64_1[prediction64_1 <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Prediction of test set\n",
    "conf_matrix = confusion_matrix(y_test64_1,prediction64_1)\n",
    "sns.heatmap(conf_matrix,cmap=\"YlGnBu\",annot=True,fmt='g');\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better CNN model leveraging (64x64) grayscale photos confuses more dogs(1) as cats (0) than cats (0) as dogs (1). Further training and modification of hyperparameters should be pursued to continue to improve the accuracy of image classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
