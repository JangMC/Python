{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A financial institution is evaluating machine learning technologies for optical character recognition. The following testing has been conducted to explore what type of neural network typology and hyperparameter settings could be recommended to the financial instituition. Initial testing is on the MNIST digits dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded train-images-idx3-ubyte.gz\n",
      "Already downloaded train-labels-idx1-ubyte.gz\n",
      "Already downloaded t10k-images-idx3-ubyte.gz\n",
      "Already downloaded t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# code from 3_mnist_from_scratch-data-dump.py\n",
    "# Import MNIST data\n",
    "\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "WORK_DIRECTORY = \"C:/Users/MCJ/Documents/Northwestern/422 - Machine Learning/Assignment \\\n",
    "                  6/jump-start-mnist-scikit-learn-ann\"\n",
    "\n",
    "def maybe_download(filename):\n",
    "    \"\"\"A helper to download the data files if not present.\"\"\"\n",
    "    if not os.path.exists(WORK_DIRECTORY):\n",
    "        os.mkdir(WORK_DIRECTORY)\n",
    "    filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    else:\n",
    "        print('Already downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magic number 2051\n",
      "image count 10000\n",
      "rows 28\n",
      "columns 28\n",
      "First 10 pixels: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing:\n",
    "# Decompress the gzipped data\n",
    "# Normalize these to [-0.5, 0.5].\n",
    "# Ensure data is read properly by reading first image from test data \n",
    "\n",
    "import gzip, binascii, struct, numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with gzip.open(test_data_filename) as f:\n",
    "    # Print the header fields.\n",
    "    for field in ['magic number', 'image count', 'rows', 'columns']:\n",
    "        # struct.unpack reads the binary data provided by f.read.\n",
    "        # The format string '>i' decodes a big-endian integer, which\n",
    "        # is the encoding of the data.\n",
    "        print(field, struct.unpack('>i', f.read(4))[0])\n",
    "    \n",
    "    # Read the first 28x28 set of pixel values. \n",
    "    # Each pixel is one byte, [0, 255], a uint8.\n",
    "    buf = f.read(28 * 28)\n",
    "    image = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "  \n",
    "    # Print the first few values of image.\n",
    "    print('First 10 pixels:', image[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magic number 2051\n",
      "image count 10000\n",
      "rows 28\n",
      "columns 28\n",
      "First 10 pixels: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decompress the gzipped data\n",
    "# Normalize data to [-0.5, 0.5].\n",
    "# Ensure data is read properly by reading first image from test data \n",
    "\n",
    "import gzip, binascii, struct, numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with gzip.open(test_data_filename) as f:\n",
    "    # Print the header fields.\n",
    "    for field in ['magic number', 'image count', 'rows', 'columns']:\n",
    "        # struct.unpack reads the binary data provided by f.read.\n",
    "        # The format string '>i' decodes a big-endian integer, which\n",
    "        # is the encoding of the data.\n",
    "        print(field, struct.unpack('>i', f.read(4))[0])\n",
    "    \n",
    "    # Read the first 28x28 set of pixel values. \n",
    "    # Each pixel is one byte, [0, 255], a uint8.\n",
    "    buf = f.read(28 * 28)\n",
    "    image = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    " \n",
    "  # Print the first few values of image.\n",
    "    print('First 10 pixels:', image[:10])\n",
    "    \n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "# We'll show the image and its pixel value histogram side-by-side.\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# To interpret the values as a 28x28 image, we need to reshape\n",
    "# the numpy array, which is one dimensional.\n",
    "ax1.imshow(image.reshape(28, 28), cmap=plt.cm.Greys);\n",
    "\n",
    "ax2.hist(image, bins=20, range=[0,255]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks like the data is being brought in correctly as the image appears to be a seven and the histogram is indicating a lot of white(0), some black (255) and grayscale as everything in between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# convert the uint8 image to 32 bit floats and \n",
    "# rescale the values to be centered around 0, between [-0.5, 0.5]. \n",
    "scaled = image.astype(numpy.float32)\n",
    "scaled = (scaled - (255 / 2.0)) / 255\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(scaled.reshape(28, 28), cmap=plt.cm.Greys);\n",
    "ax2.hist(scaled, bins=20, range=[-0.5, 0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran cell, but deleted results to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magic number 2049\n",
      "label count 10000\n",
      "First label: 7\n"
     ]
    }
   ],
   "source": [
    "# unpack the test label data and ensure it is correct, expecting a 7\n",
    "with gzip.open(test_labels_filename) as f:\n",
    "    # Print the header fields.\n",
    "    for field in ['magic number', 'label count']:\n",
    "        print(field, struct.unpack('>i', f.read(4))[0])\n",
    "\n",
    "    print('First label:', struct.unpack('B', f.read(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/MCJ/Documents/Northwestern/422 - Machine Learning/Assignment 6/jump-start-mnist-scikit-learn-ann\\train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/MCJ/Documents/Northwestern/422 - Machine Learning/Assignment 6/jump-start-mnist-scikit-learn-ann\\t10k-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing data sets\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "PIXEL_DEPTH = 255\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "  \n",
    "    For MNIST data, the number of channels is always 1.\n",
    "\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "        return data\n",
    "\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reshaping parameters are correct by\n",
    "# examing the dimensions and the first two images.\n",
    "\n",
    "print('Training data shape', train_data.shape)\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(train_data[0].reshape(28, 28), cmap=plt.cm.Greys);\n",
    "ax2.imshow(train_data[1].reshape(28, 28), cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran cell, but deleted results to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/MCJ/Documents/Northwestern/422 - Machine Learning/Assignment 6/jump-start-mnist-scikit-learn-ann\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/MCJ/Documents/Northwestern/422 - Machine Learning/Assignment 6/jump-start-mnist-scikit-learn-ann\\t10k-labels-idx1-ubyte.gz\n",
      "Training labels shape (60000, 10)\n",
      "First label vector [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Second label vector [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Load full set of labels. \n",
    "# Convert input labels into a 1-hot encoding for digits 0-9\n",
    "\n",
    "NUM_LABELS = 10\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        # Skip the magic number and count; we know these values.\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return (numpy.arange(NUM_LABELS) == labels[:, None]).astype(numpy.float32)\n",
    "\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# Ensure 1-hot encoding looks accurate\n",
    "print('Training labels shape', train_labels.shape)\n",
    "print('First label vector', train_labels[0])\n",
    "print('Second label vector', train_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation shape (5000, 28, 28, 1)\n",
      "Train size 55000\n"
     ]
    }
   ],
   "source": [
    "# Create validation dataset from training set\n",
    "\n",
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "print('Validation shape', validation_data.shape)\n",
    "print('Train size', train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCJ\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# code from 3_mnist_from_scratch.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# bundle groups of examples during training for efficiency\n",
    "BATCH_SIZE = 60\n",
    "NUM_CHANNELS = 1\n",
    "SEED = 42\n",
    "\n",
    "# Where training samples and labels are fed to the graph.\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "# For the validation and test data, the entire dataset will be \n",
    "# in one constant node.\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "# The variables below hold all the trainable weights. For each, the\n",
    "# parameter defines how the variables will be initialized.\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, NUM_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# code from 3_mnist_from_scratch-data-dump.py\n",
    "# Wire variables together into a TensorFlow graph.\n",
    "\n",
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. Pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-e8e279ca72a9>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# create multiple copies of graphs for training, \n",
    "# testing, and validation.\n",
    "# \n",
    "#`train_prediction` holds the training graph, for which we use cross-entropy loss and weight regularization. \n",
    "#We'll adjust the learning rate during training -- that's handled by the `exponential_decay` operation, \n",
    "#which is itself an argument to the `MomentumOptimizer` that performs the actual training.\n",
    "\n",
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate training loop and periodically evaluate loss and error.\n",
    "# Create a session and initialize variables defined above.\n",
    "\n",
    "# Create an interactive session to use in\n",
    "# subsequent code cells.\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "# Use newly created session as the default for \n",
    "# subsequent operations.\n",
    "s.as_default()\n",
    "\n",
    "# Initialize all the variables we defined above.\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# create scoring function.\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = numpy.zeros([10, 10], numpy.float32)\n",
    "    bundled = zip(numpy.argmax(predictions, 1), numpy.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from 3_mnist_from_scratch-data-dump.py\n",
    "# Define a loop to go through and train the data.\n",
    "# Includes printing of loss and error at 100 step intervals.\n",
    "# warning: takes a lot of time to go through the iterations\n",
    "# if you are running this code. \n",
    "\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "# Train over the first 1/4th of the training set.\n",
    "steps = train_size // BATCH_SIZE\n",
    "for step in range(steps):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the graph and fetch some of the nodes.\n",
    "    _, l, lr, predictions = s.run(\n",
    "      [optimizer, loss, learning_rate, train_prediction],\n",
    "      feed_dict=feed_dict)\n",
    "    \n",
    "    # Print out the loss periodically.\n",
    "    if step % 100 == 0:\n",
    "        error, _ = error_rate(predictions, batch_labels)\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        print('Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "              validation_prediction.eval(), validation_labels)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 1.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the results using the test set.\n",
    "# To identify mispredictions, include the raw count of each \n",
    "# (prediction, label) pair in the confusion matrix.\n",
    "\n",
    "test_error, confusions = error_rate(test_prediction.eval(), test_labels)\n",
    "print('Test error: %.1f%%' % test_error)\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(False)\n",
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.yticks(numpy.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');\n",
    "\n",
    "for i, cas in enumerate(confusions):\n",
    "    for j, count in enumerate(cas):\n",
    "        if count > 0:\n",
    "            xoff = .07 * len(str(count))\n",
    "            plt.text(j-xoff, i+.2, int(count), fontsize=9, color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are generally accurate, with some errors of predicting 3's and 6's as 5's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Run complete. data objects sent to binary file  mnist_data.pickle\n"
     ]
    }
   ],
   "source": [
    "# code 4_mnist_from_scratch-data-dump.py\n",
    "# Export the train, validation, and test objects to a binary file \n",
    "# using Python pickle dump.\n",
    "\n",
    "import pickle  \n",
    "\n",
    "# define collection of objects to export as binary file using pickle.\n",
    "data = {\n",
    "    'train_data': train_data,\n",
    "    'train_labels': train_labels,\n",
    "    'validation_data': validation_data,\n",
    "    'validation_labels': validation_labels,\n",
    "    'test_data': test_data,\n",
    "    'test_labels': test_labels}\n",
    "\n",
    "# write to binary file\n",
    "with open('mnist_data.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('\\n Run complete. data objects sent to binary file  mnist_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift to scikit-learn for subsequent modeling\n",
    "# due to errors obtained in earlier section \n",
    "# check data for pickle dump\n",
    "print('\\ntrain_data object:', type(train_data), train_data.shape)    \n",
    "print('\\ntrain_labels object:', type(train_labels),  train_labels.shape)  \n",
    "print('\\nvalidation_data object:', type(validation_data),  validation_data.shape)  \n",
    "print('\\nvalidation_labels object:', type(validation_labels),  validation_labels.shape)  \n",
    "print('\\ntest_data object:', type(test_data),  test_data.shape)  \n",
    "print('\\ntest_labels object:', type(test_labels),  test_labels.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran cell, but deleted results to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from 5_mnist_from_scratch_data_load-v001.py   \n",
    "# Imports the the train, validation, and test objects \n",
    "# from a binary file <mnist_data.pickle>\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "with open('mnist_data.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# extract objects from the dictionary object data\n",
    "train_data = data['train_data']\n",
    "train_labels = data['train_labels'] \n",
    "validation_data = data['validation_data'] \n",
    "validation_labels = data['validation_labels'] \n",
    "test_data = data['test_data'] \n",
    "test_labels = data['test_labels']  \n",
    "    \n",
    "# check data from pickle load\n",
    "print('\\ntrain_data object:', type(train_data), train_data.shape)    \n",
    "print('\\ntrain_labels object:', type(train_labels),  train_labels.shape)  \n",
    "print('\\nvalidation_data object:', type(validation_data),  validation_data.shape)  \n",
    "print('\\nvalidation_labels object:', type(validation_labels),  validation_labels.shape)  \n",
    "print('\\ntest_data object:', type(test_data),  test_data.shape)  \n",
    "print('\\ntest_labels object:', type(test_labels),  test_labels.shape)  \n",
    "\n",
    "print('\\ndata input complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran cell, but deleted results to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from mnist_from_scratch_scikit-learn-ann-v001.py\n",
    "# restructure input files to use with Scikit-Learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# convert binary digits to digits 0-9\n",
    "def label_transform(y_in):\n",
    "    for i in range(len(y_in)):\n",
    "        if (y_in[i] == 1): return i\n",
    "\n",
    "y_train = []    \n",
    "for j in range(train_labels.shape[0]):\n",
    "    y_train.append(label_transform(train_labels[j,]))  \n",
    "y_train = np.asarray(y_train)    \n",
    "\n",
    "y_validation = []    \n",
    "for j in range(validation_labels.shape[0]):\n",
    "    y_validation.append(label_transform(validation_labels[j,]))  \n",
    "y_validation = np.asarray(y_validation)    \n",
    "\n",
    "y_test = []    \n",
    "for j in range(test_labels.shape[0]):\n",
    "    y_test.append(label_transform(test_labels[j,]))  \n",
    "y_test = np.asarray(y_test)    \n",
    "    \n",
    "# 28x28 matrix of entries converted to vector of 784 entries    \n",
    "X_train = train_data.reshape(55000, 784)\n",
    "X_validation = validation_data.reshape(5000, 784)    \n",
    "X_test = test_data.reshape(10000, 784)    \n",
    "\n",
    "# check data intended for Scikit Learn input\n",
    "print('\\nX_train object:', type(X_train), X_train.shape)    \n",
    "print('\\ny_train object:', type(y_train),  y_train.shape)  \n",
    "print('\\nX_validation object:', type(X_validation),  X_validation.shape)  \n",
    "print('\\ny_validation object:', type(y_validation),  y_validation.shape)  \n",
    "print('\\nX_test object:', type(X_test),  X_test.shape)  \n",
    "print('\\ny_test object:', type(y_test),  y_test.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_expanded object: <class 'numpy.ndarray'> (60000, 784)\n",
      "\n",
      "y_train_expanded object: <class 'numpy.ndarray'> (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scikit Learn MLP Classification does validation internally, \n",
    "# Eliminate separate validation dataset by \n",
    "# Combining the train and validation sets.\n",
    "\n",
    "X_train_expanded = np.vstack((X_train, X_validation))\n",
    "y_train_expanded = np.vstack((y_train.reshape(55000,1), y_validation.reshape(5000,1)))\n",
    "\n",
    "print('\\nX_train_expanded object:', type(X_train_expanded),  X_train_expanded.shape)  \n",
    "print('\\ny_train_expanded object:', type(y_train_expanded), y_train_expanded.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-2-Layers-5-Nodes/Layer-identity\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 5), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 74.480409\n",
      "\n",
      "Training set accuracy: 0.903600\n",
      "\n",
      "Test set accuracy: 0.900900\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-2-Layers-10-Nodes/Layer-identity\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 94.075236\n",
      "\n",
      "Training set accuracy: 0.928317\n",
      "\n",
      "Test set accuracy: 0.919600\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-4-Layers-5-Nodes/Layer-identity\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 5, 5, 5), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 119.934976\n",
      "\n",
      "Training set accuracy: 0.874533\n",
      "\n",
      "Test set accuracy: 0.868500\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-4-Layers-10-Nodes/Layer-identity\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(10, 10, 10, 10), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 93.002103\n",
      "\n",
      "Training set accuracy: 0.927217\n",
      "\n",
      "Test set accuracy: 0.917400\n"
     ]
    }
   ],
   "source": [
    "#run through models\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "names = ['ANN-2-Layers-5-Nodes/Layer-identity',\n",
    "         'ANN-2-Layers-10-Nodes/Layer-identity',\n",
    "         'ANN-4-Layers-5-Nodes/Layer-identity',\n",
    "         'ANN-4-Layers-10-Nodes/Layer-identity']\n",
    "\n",
    "layers = [2, 2, 4, 4]\n",
    "nodes_per_layer = [5,10,5, 10]\n",
    "treatment_condition = [(5, 5), \n",
    "                       (10, 10), \n",
    "                       (5, 5, 5, 5), \n",
    "                       (10, 10, 10, 10)] \n",
    "\n",
    "    \n",
    "methods = [MLPClassifier(hidden_layer_sizes=treatment_condition[0], activation='identity', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', \n",
    "              learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=500, shuffle=True, \n",
    "              random_state=RANDOM_SEED, \n",
    "              tol=0.001, verbose=False, warm_start=False, momentum=0.9, \n",
    "              nesterovs_momentum=True, early_stopping=False, \n",
    "              validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[1], activation='identity', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', \n",
    "              learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=500, shuffle=True, \n",
    "              random_state=RANDOM_SEED, \n",
    "              tol=0.001, verbose=False, warm_start=False, momentum=0.9, \n",
    "              nesterovs_momentum=True, early_stopping=False, \n",
    "              validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[2], activation='identity', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', \n",
    "              learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=500, shuffle=True, \n",
    "              random_state=RANDOM_SEED, \n",
    "              tol=0.001, verbose=False, warm_start=False, momentum=0.9, \n",
    "              nesterovs_momentum=True, early_stopping=False, \n",
    "              validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[3], activation='identity', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', \n",
    "              learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=500, shuffle=True, \n",
    "              random_state=RANDOM_SEED, \n",
    "              tol=0.001, verbose=False, warm_start=False, momentum=0.9, \n",
    "              nesterovs_momentum=True, early_stopping=False, \n",
    "              validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08)]\n",
    " \n",
    "index_for_method = 0 \n",
    "training_performance_results = []\n",
    "test_performance_results = []\n",
    "processing_time = []\n",
    "   \n",
    "for name, method in zip(names, methods):\n",
    "    print('\\n------------------------------------')\n",
    "    print('\\nMethod:', name)\n",
    "    print('\\n  Specification of method:', method)\n",
    "    start_time = time.clock()\n",
    "    method.fit(X_train, y_train)\n",
    "    end_time = time.clock()\n",
    "    runtime = end_time - start_time  # seconds of wall-clock time \n",
    "    print(\"\\nProcessing time (seconds): %f\" % runtime)        \n",
    "    processing_time.append(runtime)\n",
    "\n",
    "    # mean accuracy of prediction in training set\n",
    "    training_performance = method.score(X_train_expanded, y_train_expanded)\n",
    "    print(\"\\nTraining set accuracy: %f\" % training_performance)\n",
    "    training_performance_results.append(training_performance)\n",
    "\n",
    "    # mean accuracy of prediction in test set\n",
    "    test_performance = method.score(X_test, y_test)\n",
    "    print(\"\\nTest set accuracy: %f\" % test_performance)\n",
    "    test_performance_results.append(test_performance)\n",
    "                \n",
    "    index_for_method += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Experiment: Scikit Learn Artificial Neural Networks\n",
      "\n",
      "                            Method Name  Layers  Nodes per Layer  \\\n",
      "0   ANN-2-Layers-5-Nodes/Layer-identity       2                5   \n",
      "1  ANN-2-Layers-10-Nodes/Layer-identity       2               10   \n",
      "2   ANN-4-Layers-5-Nodes/Layer-identity       4                5   \n",
      "3  ANN-4-Layers-10-Nodes/Layer-identity       4               10   \n",
      "\n",
      "   Processing Time  Training Set Accuracy  Test Set Accuracy  \n",
      "0        74.480409               0.903600             0.9009  \n",
      "1        94.075236               0.928317             0.9196  \n",
      "2       119.934976               0.874533             0.8685  \n",
      "3        93.002103               0.927217             0.9174  \n"
     ]
    }
   ],
   "source": [
    "# aggregate the results for final report\n",
    "# using OrderedDict to preserve the order of variables in DataFrame    \n",
    "from collections import OrderedDict  \n",
    "\n",
    "results = pd.DataFrame(OrderedDict([('Method Name', names),\n",
    "                        ('Layers', layers),\n",
    "                        ('Nodes per Layer', nodes_per_layer),\n",
    "                        ('Processing Time', processing_time),\n",
    "                        ('Training Set Accuracy', training_performance_results),\n",
    "                        ('Test Set Accuracy', test_performance_results)]))\n",
    "\n",
    "print('\\nBenchmark Experiment: Scikit Learn Artificial Neural Networks\\n')\n",
    "print(results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neural network structures were explored within a benchmark experiment, with two levels on each of two experimental factors.  For example, two and four layers along with five and ten nodes per layer. While many MLPClassifier models were run with different nodes, layers, solvers, iterations, etc. to fine-tune the models and adjust for bias and variance, only one result is shown in this report. For example, models with different \n",
    "activation functions for the hidden layer were run initially (relu, tanh, identity, and logistic) and the identity function showed the most promise and moved forward,\n",
    "\n",
    "From this study, we can conclude that more work needs to be done to come up with the optimal solution. Most of the fine-tuning has been a result of trial and error, and with more time, a better solution could be found. I would recommend to the financial institution to continue to walk down the path of evaluating an artificial neural network using the MLPClassifer in Python Scikit-learn with 2 layers and 10 nodes per layer, leveraging the identity funtion for the hidden layers along with 'adam' solver for weight optimization. A model with a similar level of accuracy, is the 4 layer, 10 nodes per layer model, suggesting that the additional layers may not be very helpful to include.\n",
    "\n",
    "The tolerance for the optimization along with learning_rate_init should also be explored to carry out to .0001 or .00001 as when I ran these elements, the accuracy tended to improve, but the processing time lengthened. On a more powerful machine, these elements could take less time and help improve the overall model. However, the elements that I have identified appear to be trustworthy for implementing optical character recognition as both the training and test set accuracies are high.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
